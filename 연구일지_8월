Goal: 블록 유형별 메모리 접근량 / 연산량 비교 

Steps:
train_val.prototxt               ← NOW
solver.prototxt
=> 정상 작동 test 
deploy.prototxt
streamline 으로 capture, analyze




7월 말에는
Odroid 에 caffe 설치하였음

8월 1일 (수)
178.78 서버 내 계정에 caffe 설치하였음

8월 2일 (목)
Q. 
서버에선 GPU 쓰고 Odroid에서는 CPU써도 되는지?
Deploy는 train_val과 다른 solver 만들어줘야 함?
왜 Test에도 batch size 있는지?

나중에 해결할 것
Dataset 변환 lmdb로 하는 법

지금 하는 것
MNIST Example 돌리면서 공부
train_val 과 solver 부분씩 베껴 봄

8월 4일 (토)
지금 할 일 
Odroid에서 Example 돌리면서 DS-5 사용해보기\

한 일
서버에서 Example 돌리기


8월 13일 (월)
연구 주제 다시 정리
= 다양한 구조의 필터를 단순한 네트워크 형태로 만들어
메모리 접근횟수, 캐시미스, 연산량과 실제 실행 시간을 측정해 비교한다.
(++ 이론상 Complexity를 계산해 비교한다 / 왜 이런 횟수가 나타나는지 검증한다)

* ShuffleNet에서는 Theoretically 4배, Actually 2.6배  Speedup이 이루어졌음
차이의 이유는 Memory access, other overheads로 제시
Theoretical Complexity 계산은 연산량만으로 이루어진 것?

오늘 한 일
서버에서 Training 한 Example을 Odroid에서 test 함.
gator odroid에 설치 마무리

할 일 
Odroid에서 Example 돌리면서 DS-5 사용해보기

하고있는 일
DS에서 Memory Access가 활성이 안됨 고치기

@ 서버와 Odroid간 파일 전송 어떻게?
: 서버에서는 Odroid가 안보임. Odroid에서 scp를 쓴다.

지금 cpu only로 caffe를 해놨는데
GPU도 쓰는 쪽이 나을거라고 하심.
-> MGD daemon(Mali GPU Debugger) 필요

* CPU Counter가 꺼진 채로 리눅스 빌드가 되어서
Streamline에 빈칸으로 뜨는 항목이 많았던 것.


8월 14일 (화)
어제 하윤오빠가 시킨대로 빌드하고  sudo reboot해놓고 갔더니
odroid 원격 접속이 안됨. odroid가 팬만 계속 돌린다.
커널이 4.2.0 버전이었기 때문에 device tree 문제 생김.
밀고 재설치한다.
microSD에 odroid 이미지 굽기
리눅스 커널 빌드
서버에서 make odroidxu4_defconfig 치니까 arch/x86에서 없는걸 찾아서
그냥 odroid에서 다 빌드하는게 낫겠음.
“PMU” 뭔지 모르는데 알아봐야 할듯.
이런거 안되는 버전 있으면 되는 옛날 버전 찾아서 쓰는게 낫다고.
이번에는 단순히 걍 빌드 해보고 안되면 찾아놓은 옵션 추가해서 다시 빌드해볼 것
gator.ko 만들기
Cross compiler  - tool chain 이란것을 받아야 되는 듯.
기본으로 gcc-4.7-arm-linux-gnueabi가 설치되어 있었다. 이걸 이용? 어떻게?

http://xenostudy.tistory.com/485
여기서 본 대로 크로스컴파일 할 Makefile에 ARCH와 CROSS_COMPILE 추가
PATH 추가는 기본적으로 되어있는거 같으니까 따로 안하고 간다

make -C /home/odroid/linux/ M=`pwd` ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- modules
` != ‘
gator daemon 만들기

8월 15일 (수)
gatord 만들었고 streamline에서 빈칸이던게 정상적으로 채워지는거 확인.
이제 다시 caffe install 해야 함.
opencl-caffe?

8월 16일 (목)
caffe와 opencl-caffe를 둘다 설치해보기로 한다.
opencv에서 막히고있음


[ 37%] Building CXX object modules/core/CMakeFiles/opencv_core.dir/src/lpsolver.cpp.o
virtual memory exhausted: Cannot allocate memory
modules/core/CMakeFiles/opencv_core.dir/build.make:209: recipe for target 'modules/core/CMakeFiles/opencv_core.dir/src/lpsolver.cpp.o' failed
make[2]: *** [modules/core/CMakeFiles/opencv_core.dir/src/lpsolver.cpp.o] Error 1
CMakeFiles/Makefile2:2255: recipe for target 'modules/core/CMakeFiles/opencv_core.dir/all' failed
make[1]: *** [modules/core/CMakeFiles/opencv_core.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
virtual memory exhausted: Cannot allocate memory
[ 37%] Building CXX object modules/sfm/src/libmv/libmv/multiview/CMakeFiles/multiview.dir/robust_estimation.cc.o
modules/sfm/src/libmv/libmv/multiview/CMakeFiles/multiview.dir/build.make:206: recipe for target 'modules/sfm/src/libmv/libmv/multiview/CMakeFiles/multiview.dir/panography_kernel.cc.o' failed
make[2]: *** [modules/sfm/src/libmv/libmv/multiview/CMakeFiles/multiview.dir/panography_kernel.cc.o] Error 1
make[2]: *** Waiting for unfinished jobs....
c++: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.
modules/sfm/src/libmv/libmv/multiview/CMakeFiles/multiview.dir/build.make:86: recipe for target 'modules/sfm/src/libmv/libmv/multiview/CMakeFiles/multiview.dir/euclidean_resection.cc.o' failed
make[2]: *** [modules/sfm/src/libmv/libmv/multiview/CMakeFiles/multiview.dir/euclidean_resection.cc.o] Error 4


8월 17일 (금)
opencv 3.4.0 깐다고 뻘짓했는데…
odroid에 원래 2버전 깔려 있다ㅜ
https://community.arm.com/graphics/f/discussions/8600/can-arm-mali-gpu-run-tensorflow-or-caffe-deep-learning-model
“CNN on Mali is a joke (my experience with Mali T628)”
Mali GPU로 딥러닝 돌리려면 ARM Library로 돌려야 하고 하윤오빠랑 광배가 하는 일. 
그거는 일단 caffe CPU only로 해본 후에 하라고 함.

<ODROID-XU4에서 caffe 까는 법>
http://iamyoonkim.tistory.com/6
여기서 CUDA와 opencv를 빼고 설치를 한다.
Makefile.config에서  
- CPU_ONLY를 켠다.
- INCLUDE_DIRS := (…)  /usr/include/hdf5/serial/ 추가
- LIBRARY_DIRS := (...) /usr/lib/arm-linux-gnueabihf/hdf5/serial/ 추가
apt-get install liblmdb-dev :: leveldb 적혀있어서 했는데 lmdb도 해줘야되네
make clean부터 따라한다. 중간에 뻑나면 clean부터 다시한다.
caffe 설치 완료


8월 18일 (토)
할 일 
Simple models 
[ design - implement - train - test - inference ]
Streamline에서 어떤 수치를 어떻게 읽을것인지?

놀 시기 아닌데 너무 놀았다. 

8월 20일 (월)
내일 있을 코딩테스트 대비 C언어 공부

8월 21일 (화)
코딩테스트 봤다. 기력이 빠지긴 하는데 ShuffleNet caffe 좀 보고 가야지
channel shuffle 간단 구현 방법: reshape - transpose - flatten 이라고 좀 쓸데없는 지식

>  Shuffle Layer <
layer {
  name: "shuffle1"
  type: "ShuffleChannel"
  bottom: "concat1"
  top: "shuffle1"
  shuffle_channel_param {
    group: 2
  }
}
//  shuffle_channel_param {    group: 2  인데  코드에서는 3 slice.로나누나


8월 22일 (수)
MKL2017 : Intel Caffe Engine


8월 23일 (목)
Dataset은 MNIST 혹은 CIFAR-10 으로 한다.
MNIST는 caffe example에 있다.

transform_param { scale : 0.3xxxx } 어떻게 줘야할지???
-> MNIST의 한 픽셀이 가지는 값 0~255를 0~1으로 매핑하는 scale factor임

 ★ Input layer와 Block 사이에 채널 뻥튀기 하는 conv layer가 하나 있어야 된다.

squeezenext가 hardware-aware design 이라는거 무슨뜻?
: multi-processor embedded system 에서의 performance simulation 으로 architecture optimize
optimization 과정에서 뭐를 어느 쪽으로 바꾼걸까?


1.0-G-SqNxt-23 train_val.prototxt

1: 64 (1) 7/7 - pool - 
2: 32 (1) 1/1
3: 16 (2) 1/1
4: 8 (1)  1/1
5: 16 (2) 1*3
6: 16 (2) 3*1
7: 32 (1) 1/1
Eltwise1: SUM, from 2 & 7

8: 16 (2) 1/1
9: 8 (1) 1/1
10: 16 (2) 3*1
11: 16 (2) 1*3

1.0-SqNxt-23 train_val.prototxt
1: 64 (1) 7/7
 - pool - 
2: 32 (1) 1/1
3: 16 (1) 1/1
4: 8 (1) 1/1 
5: 16 (1) 1*3
6: 16 (1) 3*1
7: 32 (1) 1/1
Eltwise1: SUM, from 2 & 7
여기서는 Group Conv가 아예 없음.

Eltwise : Element wise. Res-connection의 이전 레이어 더하기 기능
FC layer, Softmax(loss), Accuracy 일단 베껴놓긴 했는데 이해가 덜 됐다.
Accuracy layer가 bottom을 FC로 함. Acc에서는 Softmax 가 필요없고 걍 max만 보면 되어서?


8월 26일 (일)
CIFAR-10 : 32*32, 50000 train + 10000 test , 
MNIST : 28*28

일단 MNIST 버전으로 만들어본다. 
 transform_param{
    scale: 0.00390625
}

block전 conv layer 파라미터 설정중
num_output: 일단 32
stride = 1
bias_term = default
kernel_size = 5
pad = 2 or default ?  ⇒ MNIST 에서는 default도 괜찮을 것. CIFAR에서는 2로 주자.

Local Response Normalization 설명 (layer type: ”LRN”)
출처: http://nmhkahn.github.io/Casestudy-CNN

한 점 (x, y)에 대해 어떤 필터를 주위 필터 값을 반영해서 정규화

Generalization 에러를 줄이기 위해 정규화를 한다.
aix,yax,yi 는 2D 이미지에서 (x, y) 위치에서 ii 번째 커널(필터)을 의미한다. k,n,α,βk,n,α,β 는 hyper-parameter로 이 논문에서는 k=2,n=5,α=10−4,β=0.75k=2,n=5,α=10−4,β=0.75 로 설정했다.
이 정규화는 같은 spatial 위치에 있는 nn 만큼 adjust한 필터들의 square-sum을 이용하여 정규화 하는 것이다. 예를들어 n=5n=5 라면, (2, 3)번째 픽셀에 위치하는 5번째 필터는 그 위치의 3~7번째 필터에 해당하는 결과값을 이용하여 정규화 한다.
참고로 지금은 성능상 큰 이점이 없어서 잘 사용하지는 않는다.


Q. scale layer 무슨 일 하는거?


대강 simple sqznxt 만들어 놨다. 빠진 parameter 있는지 체크할 것.

8월 27일 (월)
Q. scale layer 무슨 일 하는가?
https://stackoverflow.com/questions/37410996/scale-layer-in-caffe
input blob이 하나인 경우, 다음 scale factor a와 constant b를 학습함.
 r’ = ar + b 

Q. lr_policy: “inv” or “poly” 
example lenet의 solver를 따라서 “inv”로 해보기.
 
sqznext simple ver. 에서  train_test, solver, train.sh까지 함.
- Test accuracy 0.982인데 충분한가? - 데이터셋 같은 네트워크끼리만 맞춰주자


이제 deploy를 만들어 보려고 하는데 필요할까?  -> SHIFTNet 참고하자
- Arithmetic Intensity: ratio of FLOPs to memory accesses - 여기선 어떻게 쟀나
       -> 실제로 측정하는 내용은 없었다. 
	standard, depthwise conv 에서 ratio 계산만 했음.
- deploy.prototxt 만드는건 쉬운데 사용할 때 python 파일을 만들어야 한다.
만드는방법: https://github.com/BVLC/caffe/wiki/Using-a-Trained-Network:-Deploy
http://shengshuyang.github.io/A-step-by-step-guide-to-Caffe.html 여기도 참고





[Goal에 추가]:
이론적 Arithmetic Intensity와 실제 Odroid에서의  Arith. Intensity 비교 -> 왜 차이가 날지
-> 이론적 Intensity에서는 # of memory accesses 를 단순히 숫자(param, activation, grad)의 총 개수로 치는가? 한 parameter가 여러번 접근되는데 중도에 cache에서 추방되는 일은 없나?   -> Shift에서의 standard conv & depthwise ratio 식을 다시 보고 판단.

( Memory reuse rate를 되도록 유지하면서 FLOPs를 줄이는 필터 유형 찾기 )

Q. test를 batch size 1, iter 1로 해서 하면 inference와 연산이 같지 않은가?
batch size 는 train_test.prototxt 에서 바로 수정 됨
iter 는? http://caffe.berkeleyvision.org/tutorial/interfaces.html 에서 보면
-iterations ## 인자를 줄 수 있다. 
→ batch 1 iter 1 만들기 된다.

※ caffe time : 위 링크에 들어가보면 있음. layer별 시간 재어준다.

현상 파악을 위해 Shift, Xception, ShuffleNet 논문에서 언급한 부분을 보고 있다.
Depthwise conv 연산만 이 문제를 일으킴?
1x1 conv도 parameter는 Depthwise와 같은 횟수만큼 reuse 될텐데 (stride가 둘다 1이라면), activation의 reuse는 어떻게 다를까?



***** 일단 다른 모듈 쓴 simple_train_test 만들기부터 집중하도록 한다.*****

ShuffleNet - MNIST - train_test 만드는 중
https://github.com/farmingyard/ShuffleNet 이용해서 할 일 
layer.cpp, .hpp 파일을 받아서 caffe에 추가한다.
여기서 정의한 shufflenet_1x_g3_deploy.prototxt 에서 블록을 가져온다.

Q. ShuffleNet 옆에 2x는 무슨 의미?
”ShuffleNet 1X”: baseline network
”ShuffleNet sX” means scaling the number of filters in ShuffleNet 1X by s times thus overall complexity will be roughly s^2 times of ShuffleNet 1.

ShuffleNet deploy 파일에 이름이 resx인 레이어가 등장하는데, ResNeXt인 듯함.
일단 train_test 짓고, ResNeXt 나중에 살펴볼 것.



Stage2의 첫번째 1x1 layer는 Gconv 안함.
지금 참고하는 코드에서는 g=3으로 했고, Stage2의 첫번째 블록에선 Add가 아닌 Concat을 한다. Elt 기준으로 PPT 그림에 맞는 블록을 잘라내서, 내 네트워크에 붙일 것.



Finding ShuffleNet Block in 

resx1_concat
resx1_concat_relu
------------------------- 여기서부터 내 네트워크에 포함 
resx2_conv1 : 1*1, g=3, #=60
resx2_conv1_bn
resx2_conv1_scale
resx2_conv1_relu
shuffle2 : g=3
resx2_conv2 : 3*3, Depth, #=60, s=1
resx2_conv2_bn
resx2_conv2_scale
resx2_conv3 : 1*1, g=3, #=240
resx2_conv3_bn
resx2_conv3_scale
resx2_elewise <- resx1_concat, resx2_conv3
resx2_elewise_relu
----------------------------


 
8월 28일 (화)
어제 하던 ShuffleNet 만들기 이어하기
Q: “왜 기존 모델 안돌리고 따로 축소판 만들어서 해”
-> 같은 조건을 맞추고 비교하기 위해서
   맞출 조건 예시: # of Weights - whole network? block 내에서만?
   일단 Accuracy는 무시한다. CIFAR-10에 맞춘다.

conv1 (Gconv) - conv2 (Depth) - conv3 (Gconv)
Depth는 in과 out channel 수가 같다. g = # of channels 인 conv로도 볼 수 있다.

>> ShuffleNet 현황 >> 
Done
train_test 작성
test.sh, train.sh 수정하기
solver 수정하기
.cpp, .hpp layer 정보 파일 붙이고, caffe 다시 make
    위치) .hpp : ~/caffe/include/caffe/layers$
             .cpp, .cu : ~/caffe/src/caffe/layers
ConvolutionDepthwise layer가 caffe에 없음. 등록하거나, 고친다.
train, test 쉘 파일 실행 확인
To do
Weight 개수 맞추기

Q. 맨 처음 conv layer 직후에는 relu가 필요없는가?
 -> MNIST에서는 안 써도 상관 없는 듯 하다. 
     lenet은 conv-pool을 두번씩 거치고 fc뒤에 relu 붙임.
     그리고 이건 accuracy에 관한 문제이므로 지금 알아볼 필요 없음.

Q. ShuffleNet, SqueezeNext Full Network 는 Odroid에서도 돌아갈 크기인가?

Q. 내가 가져온게 ShuffleNet의 기본적인 모듈이 맞는건가?
논문의 내가 선택한 unit 옆에 stride = 2인 unit이 있는데, 부가적인 것으로 보인다.



>> ResNet 축소 모델 만들기 >> 
bottle neck module이 적용된 block을 사용한다. 

Done
train_test 작성
test.sh, train.sh 수정하기
solver 수정하기
train, test 쉘 파일 실행 확인
To do 
weight 개수 조절하기

** ResNet-50-deploy.prototxt 구조
conv1 : #=64, 7*7, s=2
conv1_bn
conv1_scale
conv1_relu
pool1
------------------------------------------------
a) res2a_branch1 conv : #=256, 1*1, s=1    <- pool1
 - bn
 - scale
b) res2a_branch2a conv : #=64, 1*1, s=1    <- pool1
 - bn
 - scale
 - relu
    res2a_branch2b conv : #=64, 3*3, s=1
 - bn
 - scale
 - relu
    res2a_branch2c conv : #=256, 1*1, s=1
 - bn
 - scale

res2a Eltwise <- res2a_branch1,  res2a_branch2c
res2a relu
----------------------------------------------- 처음 블록이라 변형이 있음.
res2b_branch2a conv : #=64, 1*1, s=1
 - bn
 - scale
 - relu
res2b_branch2b conv : #=64, 3*3, s=1
 - bn
 - scale
 - relu
res2b_branch2c conv : #=256, 1*1, s=1
 - bn
 - scale

res2b Eltwise <- res2a Eltwise, res2b_branch2c
----------------------------------------------- 이 블록을 가져온다.


Parameter 수 계산
◆  Simple ResNet
In Block
#in_channels = 4a
res2b_branch2a (1*1, #4a->a)      = (1*1) * 4a * a
res2b_branch2b (3*3, #a->a)        = (3*3) * a * a
res2b_branch2c (1*1, #a->4a)      =(1*1) * a * 4a
Total = 4aa+9aa+4aa = 17aa
Q. Batch Norm에도 learned parameter 있지 않나?
 => Feature map의 각 점에 대해서 gamma, beta 학습하는 거라면 값이 유의미하게 클 것. 근데 이건 일단 시간 부족해서 랩세미나 후에 조사한다.
Out Block
Conv1 (5*5, #3->4a) = (5*5) * 3 * 4a = 300a
fc (#in = ??? -> feature map 크기 알아야 함 -> dataset 지정해야 함
     -> 일단 다 CIFAR-10 버전으로 바꾸자
     #out = 10)

◆ Simple ShuffleNet
In Block
resx2_conv1 (1*1, g=3, #4a->a)  = (1*1) * (4a/3) * a
resx2_conv2 (3*3, g=a, #a->a)  = (3*3) * a
resx2_conv3 (1*1, g=3, #a->4a) = (1*1) * (a/3) * 4a
Total = 9a + 8/3aa = 11.3 aa
Out Block
Conv1 (5*5, #3->4a) = (5*5) * 3 * 4a = 300a
fc (#in= ???, #out=10) 

◆ Simple SqueezeNext
In Block
Conv2 (1*1, #4a->2a) = 8aa
Conv3 (1*1, #2a->a) = 2aa
Conv4 (1*3, #a->2a) = 6aa
Conv5 (3*1, #2a->2a) = 6aa
Conv6 (1*1, #->4a) = 4aa
Total = 26 aa
Out Block
Conv1 (5*5, #3->4a) = (5*5) * 3 * 4a = 300a
fc (#in= ???, #out=10) 


8월 29일 (수)
Deploy 하기

1. deploy.prototxt 만들기 (간단함) 
   https://github.com/BVLC/caffe/wiki/Using-a-Trained-Network:-Deploy
2. caffe/python/classify.py 에서 인자를 줘서 inference 할 수 있는 듯.
3. classify.py 활용하려면 mean.npy 파일이 필요.  mean.binaryproto로부터 변환하기
   https://github.com/BVLC/caffe/issues/290
   여기 나온 코드로 convert_protomean.py 파일 만들어서 변환했다.
4. .caffemodel.h5 파일 넣어도 classify가 정상 작동 하는지 확인하자.
   => .h5가 문제가 아니라 “cannot identify image file” 문제가 난다
   참고) https://github.com/python-pillow/Pillow/issues/1006
    문제원인: HTML주소를 이미지 주소로 착각하고 wget을 써서…. 헐
     $ file sampleimg8b.png 커맨드를 써서 파일 타입을 알아내 주심.
     그리고 카페 설치 시 http://www.whydsp.org/316 
     여기서 make distribute 도 해야 되는거 였나봄.
5. Output file 어떻게 해석? <<<<<<<<< NOW HERE



Q. MNIST에서 CIFAR-10으로 옮겨오면서 solver에 snapshot_format: HDF5 추가 ← 뭔 의미? 
